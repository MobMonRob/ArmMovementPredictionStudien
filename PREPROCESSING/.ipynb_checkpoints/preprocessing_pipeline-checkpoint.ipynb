{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, pandas as pd, numpy as np, shutil\n",
    "from math import cos, sin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Raw-Umbenennung\n",
    "Benennt die Dateien in einheitliche Namen um, Ausgangspunkt müssen dazu die \"takeover\"-Namen sein (darf nicht mehrmals ausgeführt werden).\n",
    "Fehlerhafte Datumsangaben werden korrigiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('../DATA/0_raw')\n",
    "\n",
    "for file in files:\n",
    "    fileNumber = file.split(\"_takeover\",1)[0]\n",
    "    if len(fileNumber) > 12: #dont repeat script\n",
    "        break\n",
    "    \n",
    "    year = fileNumber[0:4]\n",
    "    if year != '2020' and year != '2021':\n",
    "        fileNumber = fileNumber.replace(year,'2020',1)\n",
    "    \n",
    "    if file.find(\"RGraspPhase\") != -1:\n",
    "        side=\"R\"\n",
    "    else:\n",
    "        side=\"L\"\n",
    "        \n",
    "    oldpath = r'../DATA/0_raw/'+file\n",
    "    newpath = r'../DATA/0_raw/'+str(fileNumber)+'_raw_'+side+'.csv'\n",
    "    os.rename(oldpath,newpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Interpolation\n",
    "Kubische Interpolation für bis zu 3 NaNs. Erstellt neue csv-Dateien im interpolated-Ordner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('../DATA/0_raw')\n",
    "\n",
    "length_of_file = {}\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv('../DATA/0_raw/'+file, sep = ';') \n",
    "    interpolated_df = df.interpolate(method='cubic',limit=3,limit_area='inside') \n",
    "    fileNumber = file.split(\"_raw\",1)[0]\n",
    "    side = file[-5]\n",
    "    filename = '../DATA/1_interpolated/'+str(fileNumber)+'_interpolated_'+side+'.csv'\n",
    "    interpolated_df.to_csv(filename, sep = ';', float_format='%.4f', na_rep='NaN', index=False)\n",
    "    length_of_file.update({filename:len(interpolated_df)})\n",
    "\n",
    "pd.DataFrame({\"Dateilängen\": list(length_of_file.values())}).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Glätten\n",
    "Glätten mithilfe des SavGol-Filters. Erstellt neue csv-Dateien im smoothed-Ordner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../DATA/1_interpolated/\"\n",
    "\n",
    "for f in os.listdir('../DATA/2_smoothed'):\n",
    "    os.remove(os.path.join('../DATA/2_smoothed', f))\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    dataset = pd.read_csv(directory + file, sep=\";\")\n",
    "    smoothed_dataset = savgol_filter(dataset, 13, 3, axis=0)\n",
    "    if '_R' in file:\n",
    "        right_or_left = \"R\"\n",
    "    if '_L' in file:\n",
    "        right_or_left = \"L\"\n",
    "    column_names = [\n",
    "        \"{r_l}WJC_x\".format(r_l=right_or_left),\n",
    "        \"{r_l}WJC_y\".format(r_l=right_or_left),\n",
    "        \"{r_l}WJC_z\".format(r_l=right_or_left),\n",
    "        \"{r_l}EJC_x\".format(r_l=right_or_left),\n",
    "        \"{r_l}EJC_y\".format(r_l=right_or_left),\n",
    "        \"{r_l}EJC_z\".format(r_l=right_or_left),\n",
    "        \"{r_l}GHJC_x\".format(r_l=right_or_left),\n",
    "        \"{r_l}GHJC_y\".format(r_l=right_or_left),\n",
    "        \"{r_l}GHJC_z\".format(r_l=right_or_left)\n",
    "    ]\n",
    "    dataframe_smooth = pd.DataFrame(\n",
    "        data=smoothed_dataset,\n",
    "        columns=[column_names]\n",
    "    )\n",
    "    \n",
    "    new_filename = \"../DATA/2_smoothed/\" + file.replace(\"interpolated\", \"smoothed\")\n",
    "    dataframe_smooth.to_csv(path_or_buf=new_filename, sep=';', index=False, float_format='%.4f', na_rep='NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Beschneiden\n",
    "Beschneidung anhand der Geschwindigkeit. Erstellt neue csv-Dateien im truncated-Ordner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code für diesen Schritt ist zu finden unter ```PREPROCESSING/truncate/generate_truncated_files.py```\n",
    "Dieser muss nur ausgefürt werden und speichert die verarbeiten Dateien im Ordner DATA/3_truncated ab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Statische Präfilterung\n",
    "Aussortierung von Dateien mit hoher Maximalgeschwindigkeit, ungewöhnlicher Dateilänge oder NaN-Werte im rechten Handgelenk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMUM_LEN = 85\n",
    "MAXIMUM_LEN = 150\n",
    "MAXIMUM_VELO_HAND = 5 \n",
    "\n",
    "prefiltered_files = {}\n",
    "counter = {\n",
    "    \"too_many_nan\":0,\n",
    "    \"too_short\":0,\n",
    "    \"too_long\":0,\n",
    "    \"too_fast\":0\n",
    "}\n",
    "\n",
    "for f in os.listdir('../DATA/4_prefiltered'):\n",
    "    os.remove(os.path.join('../DATA/4_prefiltered', f))\n",
    "for f in os.listdir('../DATA/98_broken_prefiltered'):\n",
    "    os.remove(os.path.join('../DATA/98_broken_prefiltered', f))\n",
    "\n",
    "for file in os.listdir('../DATA/3_truncated'):\n",
    "    side = file[-5]\n",
    "    df = pd.read_csv('../DATA/3_truncated/'+file, sep = ';') \n",
    "    \n",
    "    nan_hand = df.iloc[:,[0,1,2]].isna().sum().sum()\n",
    "    if nan_hand != 0:\n",
    "        prefiltered_files.update({file:str(nan_hand)+\" NaN-values in right hand\"})\n",
    "        counter[\"too_many_nan\"]+=1\n",
    "    else:    \n",
    "        if len(df)<MINIMUM_LEN:\n",
    "            prefiltered_files.update({file:\"To short: \"+str(len(df))})\n",
    "            counter[\"too_short\"]+=1\n",
    "        else:\n",
    "            if len(df)>MAXIMUM_LEN:\n",
    "                prefiltered_files.update({file:\"To long: \"+str(len(df))})\n",
    "                counter[\"too_long\"]+=1\n",
    "            else:\n",
    "                v_df = np.gradient(df, axis=0)\n",
    "                velos_hand = np.sqrt(np.square(v_df[:,0])+np.square(v_df[:,1])+np.square(v_df[:,2]))\n",
    "                if np.max(velos_hand)*0.12 > MAXIMUM_VELO_HAND:\n",
    "                    prefiltered_files.update({file:\"High max velocity hand: \"+str(np.max(velos_hand)*12)})\n",
    "                    counter[\"too_fast\"]+=1\n",
    "\n",
    "for file in os.listdir('../DATA/3_truncated'):\n",
    "    side = file[-5]\n",
    "    fileNumber = file.split(\"_truncated\",1)[0]\n",
    "    \n",
    "    if file in prefiltered_files.keys():       \n",
    "        broken_file = open('../DATA/3_truncated/'+file,'r')\n",
    "        appended_file = open('../DATA/98_broken_prefiltered/'+str(fileNumber)+'_broken_prefiltered_'+side+'.csv','w')\n",
    "        broken_content = broken_file.readlines()\n",
    "        appended_file.write(prefiltered_files[file]+'\\n')\n",
    "        for line in broken_content:\n",
    "            if line == broken_content[0]:\n",
    "                continue\n",
    "            appended_file.write(line)\n",
    "        broken_file.close()\n",
    "        appended_file.close()\n",
    "    else:\n",
    "        shutil.copyfile('../DATA/3_truncated/'+file,'../DATA/4_prefiltered/'+str(fileNumber)+'_prefiltered_'+side+'.csv')        \n",
    "    \n",
    "print(str(len(prefiltered_files.keys()))+\" files prefiltered: \"+str(counter[\"too_many_nan\"])+\" with to many NaN in hand, \"+str(counter[\"too_short\"])+\" too short files, \"+str(counter[\"too_long\"])+\" too long files, \"+str(counter[\"too_fast\"])+\" too fast files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Spiegelung\n",
    "Spiegelung der linken Körperhälfte und Kopie der rechten in den Ordner mirrored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code für diesen Schritt ist zu finden unter ```PREPROCESSING/mirror_left_files/mirror_files.py```\n",
    "Dieser muss nur ausgefürt werden und speichert die verarbeiten Dateien im Ordner DATA/5_mirrored ab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Verschiebung zum Ursprung\n",
    "Verschiebung der zugeschnittenen Dateien, sodass die rechte Hand im Ursprung startet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_file = {}\n",
    "\n",
    "counter_relocated=0\n",
    "\n",
    "for f in os.listdir('../DATA/6_relocated'):\n",
    "    os.remove(os.path.join('../DATA/6_relocated', f))\n",
    "\n",
    "for file in os.listdir('../DATA/5_mirrored'):\n",
    "    side = file[-5]\n",
    "    df = pd.read_csv('../DATA/5_mirrored/'+file, sep = ';')              \n",
    "    fileNumber = file.split(\"_mirrored\",1)[0]\n",
    "    relocated = pd.DataFrame(df.iloc[:,0]-df.iloc[0,0])\n",
    "    relocated[df.columns.values[1]] = df.iloc[:,1]-df.iloc[0,1]\n",
    "    relocated[df.columns.values[2]] = df.iloc[:,2]-df.iloc[0,2]\n",
    "    relocated[df.columns.values[3]] = df.iloc[:,3]-df.iloc[0,0]\n",
    "    relocated[df.columns.values[4]] = df.iloc[:,4]-df.iloc[0,1]\n",
    "    relocated[df.columns.values[5]] = df.iloc[:,5]-df.iloc[0,2]\n",
    "    relocated[df.columns.values[6]] = df.iloc[:,6]-df.iloc[0,0]\n",
    "    relocated[df.columns.values[7]] = df.iloc[:,7]-df.iloc[0,1]\n",
    "    relocated[df.columns.values[8]] = df.iloc[:,8]-df.iloc[0,2]          \n",
    "\n",
    "\n",
    "    filename = '../DATA/6_relocated/'+str(fileNumber)+'_relocated_'+side+'.csv'\n",
    "    relocated.to_csv(filename, sep = ';', float_format='%.4f', na_rep='NaN', index=False)\n",
    "    length_of_file.update({filename:len(relocated)})\n",
    "    counter_relocated+=1\n",
    "\n",
    "print(str(counter_relocated)+\" relocated\")\n",
    "pd.DataFrame({\"Dateilängen\": list(length_of_file.values())}).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Metrikbasierte Filterung\n",
    "Zunächst Einlesen der Dateien und Aufteilung in zwei Listen, in `r_files` für die rechte Körperhälfte und `l_files` für links.Außerdem Aufteilung in zwei Listen, in `r_files` für die rechte Körperhälfte und `l_files` für links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_files = []\n",
    "l_files = []\n",
    "all_files = []\n",
    "\n",
    "for file in os.listdir('../DATA/6_relocated'):\n",
    "    fileName = '../DATA/6_relocated/'+file\n",
    "    if file.find(\"R\") != -1:\n",
    "        r_files.append(fileName)\n",
    "    else:\n",
    "        l_files.append(fileName)\n",
    "    all_files.append(fileName)\n",
    "\n",
    "print(str(len(r_files))+\" right and \"+str(len(l_files))+\" left files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Ellenbogen-Handgelenk-Distanz\n",
    "Berechnet für jeden Zeitpunkt den Abstand von Handgelenk und Ellenbogen und ermittelt die Mittelwerte dieser Abstände pro Datensatz. Da dieser Mittelwert von den anatomischen Gegebenheiten der Messperson abhängt, wird die Verteilung der Mittelwerte pro Messung in einem Histogramm visualisiert. \n",
    "\n",
    "In die Liste \"elbow\" werden die Dateien geschrieben, deren durchschnittlicher Ellenbogen-Handgelenk-Abstand größer oder kleiner eines Schwellwertes ist. Als Schwellwert wird nicht standardmäßig das 95 und 5 Quantil ver, sondern dynamisch von Standarbweichung des Datensatzes abhängend bestimmen. Der Grenzwert entspricht 0.9 % der Standarabweichung.\n",
    "So werden nicht immer 10 % der Daten aussortiert, sondern je breiter die Verteilung, desto mehr.\n",
    "Allerdings lässt dies nicht zwangsläufig den Schluss auf einen Sprung in den Handgelenksdaten zu, es können auch die Ellenbogendaten gestört sein.\n",
    "\n",
    "### Statische Präfilterung\n",
    "Ausreißer werden vorab entfernt, um die dynamische Schwellwertbildung und normierte Störungsstärke nicht zu verzerren. So fließen die Ausreißer maximal mit definierten Maximalwerten in die Normierung der anderen Werte mit ein, und nicht mit extrem hohen Werten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_files_prefiltered = []\n",
    "prefiltered = {\n",
    "    \"mean\" : [],\n",
    "    \"std\": []\n",
    "}\n",
    "\n",
    "outliers_prefiltered = {}\n",
    "\n",
    "MAX_MEAN = 300\n",
    "MAX_STD = 50\n",
    "\n",
    "counter=0\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file, sep = ';')\n",
    "    dist = []\n",
    "\n",
    "    for index, row in df.iloc[:,[0,1,2,3,4,5]].iterrows():       \n",
    "        dist.append(math.sqrt((row[0]-row[3])**2 + (row[1]-row[4])**2 + (row[2]-row[5])**2))\n",
    "    series = pd.Series(dist)\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    prefiltered[\"mean\"].append(mean)\n",
    "    prefiltered[\"std\"].append(std)\n",
    "    if mean < MAX_MEAN and std < MAX_STD:\n",
    "        elbow_files_prefiltered.append(file)\n",
    "    else:\n",
    "        counter+=1\n",
    "        outliers_prefiltered.update({file:1})\n",
    "        \n",
    "pd.DataFrame(prefiltered).hist(figsize=(30, 5), bins=60)\n",
    "print(str(counter)+\" extreme outlier files, \"+str(round(100*counter/len(all_files),2))+\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markierung der Ausreißer anhand dynamischer Schwellwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances =  {}\n",
    "thresholds = {}\n",
    "outliers = {\n",
    "    \"elbow\" : {},\n",
    "    \"hand\":{},\n",
    "    \"otherHand_z\": {}\n",
    "}\n",
    "\n",
    "FACTOR = 0.009\n",
    "\n",
    "for file in elbow_files_prefiltered:\n",
    "    key = file[20:28] + '-' + file[-5]\n",
    "    df = pd.read_csv(file, sep = ';')\n",
    "    dist = []\n",
    "\n",
    "    for index, row in df.iloc[:,[0,1,2,3,4,5]].iterrows():       \n",
    "        dist.append(math.sqrt((row[0]-row[3])**2 + (row[1]-row[4])**2 + (row[2]-row[5])**2))\n",
    "    mean = pd.Series(dist).mean()\n",
    "    \n",
    "    if key not in distances:\n",
    "        distances.update({key:{}})\n",
    "    distances[key].update({file:mean})\n",
    "    \n",
    "for key in distances.keys():\n",
    "    df_distances = pd.DataFrame({key: list(distances[key].values())})\n",
    "    df_distances.hist(figsize=(10, 5), bins=80)\n",
    "    threshold_value = df_distances.std()[0] * FACTOR\n",
    "    if threshold_value >0.5:\n",
    "        threshold_value = 0.5\n",
    "        print(\"ERROR STD TO LARGE!\")\n",
    "    upper_bound = df_distances.quantile(q=(1-threshold_value))[0]\n",
    "    lower_bound = df_distances.quantile(q=threshold_value)[0]\n",
    "    thresholds.update({key:[lower_bound, upper_bound]})\n",
    "    print(key+\" std: \"+str(round(df_distances.std()[0],2))+\" treshold: \"+str(threshold_value)+\" upper: \"+str(upper_bound)+\" lower: \"+str(lower_bound))\n",
    "    \n",
    "    counter=0\n",
    "    for index, value in df_distances.iterrows():\n",
    "        if value[0] < lower_bound or value[0] > upper_bound:\n",
    "            counter+=1       \n",
    "    print(str(counter) + \" files, \"+ str(round(100*counter/len(distances[key]),2))+\" %\")\n",
    "\n",
    "idx = 0\n",
    "for file in elbow_files_prefiltered:\n",
    "    key = file[20:28] + '-' + file[-5]\n",
    "    if distances[key][file] > thresholds[key][1] or distances[key][file] < thresholds[key][0]:\n",
    "        meanOfAllValues = pd.DataFrame(distances[key].values()).mean()[0]\n",
    "        distanceToMean = abs(meanOfAllValues - distances[key][file])\n",
    "        outliers[\"elbow\"].update({file:distanceToMean})\n",
    "    idx+=1\n",
    "    \n",
    "normalized_values = list(outliers[\"elbow\"].values())\n",
    "max_normalized_value = max(normalized_values)\n",
    "min_normalized_value = min(normalized_values)\n",
    "for filePath in list(outliers[\"elbow\"].keys()):\n",
    "    outliers[\"elbow\"][filePath] = (outliers[\"elbow\"][filePath]-min_normalized_value)/(max_normalized_value-min_normalized_value)\n",
    "    \n",
    "for filePath in list(outliers_prefiltered.keys()):\n",
    "    outliers[\"elbow\"].update({filePath:outliers_prefiltered[filePath]})\n",
    "    \n",
    "print(\"Outliers: \"+str(len(outliers[\"elbow\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Handgelenk-Handgelenk-Distanz & z-Koordinatendifferenz\n",
    "Berechnet zunächst pro Messdurchlauf die durchschnittlichen Handabstände. Die Grenze für Ausreißer wird dynamisch pro Messdurchlauf definiert, sie hängt von der Standardabweichung des Durchlaufs ab.\n",
    "\n",
    "Außerdem wird die Differenz der z-Koordinaten für die Trajektorien von linker und rechter Hand gebildet, diese sollte weitesgehend 0 sein, sofern sich die Hände gleichzeitig hoch/runter bewegen. Größere Abweichungen zeigen Ausreißer an. Auch hier dynamische Schwellwertdefinition\n",
    "\n",
    "### Statische Präfilterung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_files_prefiltered = []\n",
    "prefiltered = {\n",
    "    \"z\":[],\n",
    "    \"hand\":[]\n",
    "}\n",
    "outliers_prefiltered = {\n",
    "    \"otherHand_z\":{},\n",
    "    \"hand\":{}\n",
    "}\n",
    "\n",
    "MAX_Z = 400\n",
    "MAX_HAND = 600\n",
    "count_r_with_l = 0\n",
    "\n",
    "for file in r_files:\n",
    "    startOfFile = file.split(\"R\",1)[0]\n",
    "    pathOfOtherFile = startOfFile + 'L.csv'\n",
    "\n",
    "    if os.path.isfile(pathOfOtherFile):\n",
    "        side1 = pd.read_csv(file, sep = ';').iloc[:,[0,1,2]]\n",
    "        side2 = pd.read_csv(pathOfOtherFile, sep = ';').iloc[:,[0,1,2]]\n",
    "        count_r_with_l +=2\n",
    "                \n",
    "        x_diff = []\n",
    "        y_diff = []\n",
    "        z_diff = []\n",
    "        hands = []\n",
    "        idx = 0\n",
    "        while idx<(len(side1)-1) and idx<(len(side2)-1):\n",
    "            y_diff.append(abs(side1.iloc[idx,1]-side2.iloc[idx,1]))\n",
    "            z_diff.append(abs(side1.iloc[idx,2]-side2.iloc[idx,2]))\n",
    "            x_diff.append(abs(side1.iloc[idx,0]-side2.iloc[idx,0]))\n",
    "            hands.append(math.sqrt((side1.iloc[idx,0]-side2.iloc[idx,0])**2 + (side1.iloc[idx,1]-side2.iloc[idx,1])**2 + (side1.iloc[idx,2]-side2.iloc[idx,2])**2))\n",
    "            idx+=1\n",
    "        z_mean = pd.Series(z_diff, dtype=\"float64\").mean()\n",
    "        hand_mean = pd.Series(hands, dtype=\"float64\").mean()\n",
    "        prefiltered[\"z\"].append(z_mean)\n",
    "        prefiltered[\"hand\"].append(hand_mean)\n",
    "        \n",
    "        if z_mean < MAX_Z and hand_mean < MAX_HAND:\n",
    "            hand_files_prefiltered.append(file)\n",
    "            hand_files_prefiltered.append(pathOfOtherFile)\n",
    "        elif z_mean >= MAX_Z:\n",
    "            outliers_prefiltered[\"otherHand_z\"].update({file:1})\n",
    "            outliers_prefiltered[\"otherHand_z\"].update({pathOfOtherFile:1})\n",
    "        elif hand_mean >= MAX_HAND:\n",
    "            outliers_prefiltered[\"hand\"].update({file:1})\n",
    "            outliers_prefiltered[\"hand\"].update({pathOfOtherFile:1})\n",
    "pd.DataFrame(prefiltered).hist(figsize=(20, 10), bins=50)    \n",
    "\n",
    "outliers_count = count_r_with_l - len(hand_files_prefiltered)\n",
    "print(str(outliers_count)+\" extreme outlier files, \"+str(round(100*outliers_count/count_r_with_l,2))+\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markierung der Ausreißer anhand dynamischer Schwellwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = {\n",
    "    \"x\":{},\n",
    "    \"y\":{},\n",
    "    \"z\":{},\n",
    "    \"hands\":{}\n",
    "}\n",
    "threshold = {\n",
    "    \"x\" : 0,\n",
    "    \"y\" : 0,\n",
    "    \"z\" : {},\n",
    "    \"hands\" : {}\n",
    "}\n",
    "outliers[\"otherHand_z\"].clear()\n",
    "outliers[\"hand\"].clear()\n",
    "\n",
    "r_with_l = []\n",
    "\n",
    "FACTOR = 0.0005\n",
    "FACTOR_Z = 0.0008\n",
    "\n",
    "prevFileNumber = ''\n",
    "for file in hand_files_prefiltered: \n",
    "    fileNumber = file[20:28]\n",
    "    startOfFile = file.split(\"R\",1)[0]\n",
    "    pathOfOtherFile = startOfFile + 'L.csv'\n",
    "    if os.path.isfile(pathOfOtherFile):\n",
    "        r_with_l.append(file)\n",
    "        r_with_l.append(pathOfOtherFile)\n",
    "        side1 = pd.read_csv(file, sep = ';').iloc[:,[0,1,2]]\n",
    "        side2 = pd.read_csv(pathOfOtherFile, sep = ';').iloc[:,[0,1,2]]\n",
    "                \n",
    "        x_diff = []\n",
    "        y_diff = []\n",
    "        z_diff = []\n",
    "        hands = []\n",
    "        idx = 0\n",
    "        while idx<(len(side1)-1) and idx<(len(side2)-1):\n",
    "            y_diff.append(abs(side1.iloc[idx,1]-side2.iloc[idx,1]))\n",
    "            z_diff.append(abs(side1.iloc[idx,2]-side2.iloc[idx,2]))\n",
    "            x_diff.append(abs(side1.iloc[idx,0]-side2.iloc[idx,0]))\n",
    "            hands.append(math.sqrt((side1.iloc[idx,0]-side2.iloc[idx,0])**2 + (side1.iloc[idx,1]-side2.iloc[idx,1])**2 + (side1.iloc[idx,2]-side2.iloc[idx,2])**2))\n",
    "            idx+=1\n",
    "        if fileNumber != prevFileNumber:\n",
    "            differences[\"hands\"].update({fileNumber:{}})\n",
    "            differences[\"x\"].update({fileNumber:{}})\n",
    "            differences[\"y\"].update({fileNumber:{}})\n",
    "            differences[\"z\"].update({fileNumber:{}})\n",
    "        differences[\"hands\"][fileNumber].update({file:pd.Series(hands, dtype=\"float64\").mean()})\n",
    "        differences[\"x\"][fileNumber].update({file:pd.Series(x_diff, dtype=\"float64\").mean()})\n",
    "        differences[\"y\"][fileNumber].update({file:pd.Series(y_diff, dtype=\"float64\").mean()})\n",
    "        differences[\"z\"][fileNumber].update({file:pd.Series(z_diff, dtype=\"float64\").mean()})\n",
    "        differences[\"hands\"][fileNumber].update({pathOfOtherFile:pd.Series(hands, dtype=\"float64\").mean()})\n",
    "        differences[\"x\"][fileNumber].update({pathOfOtherFile:pd.Series(x_diff, dtype=\"float64\").mean()})\n",
    "        differences[\"y\"][fileNumber].update({pathOfOtherFile:pd.Series(y_diff, dtype=\"float64\").mean()})\n",
    "        differences[\"z\"][fileNumber].update({pathOfOtherFile:pd.Series(z_diff, dtype=\"float64\").mean()})\n",
    "        prevFileNumber = fileNumber\n",
    "\n",
    "for fileNumber in differences[\"hands\"].keys():\n",
    "    hand_distances_key = fileNumber+\" hand distance\"\n",
    "    z_key = fileNumber + \" z\"\n",
    "    df= pd.DataFrame({\n",
    "        hand_distances_key : list(differences[\"hands\"][fileNumber].values()),\n",
    "        fileNumber+\" x\" : list(differences[\"x\"][fileNumber].values()),\n",
    "        fileNumber+\" y\" : list(differences[\"y\"][fileNumber].values()),\n",
    "        z_key : list(differences[\"z\"][fileNumber].values())\n",
    "    })\n",
    "    df.hist(figsize=(20, 10), bins=50)    \n",
    "    threshold_value = df.loc[:,hand_distances_key].std() * FACTOR\n",
    "    upper_bound = df.loc[:,hand_distances_key].quantile(q=(1-threshold_value))\n",
    "    lower_bound = df.loc[:,hand_distances_key].quantile(q=threshold_value)\n",
    "    threshold[\"hands\"].update({fileNumber:[lower_bound, upper_bound]})\n",
    "    threshold_value = df.loc[:,z_key].std() * FACTOR_Z\n",
    "    upper_bound_z = df.loc[:,z_key].quantile(q=(1-threshold_value))\n",
    "    threshold[\"z\"].update({fileNumber:upper_bound_z})\n",
    "    print(fileNumber+\" hand upper: \"+str(upper_bound)+\", hand lower: \"+str(lower_bound)+\", z upper: \"+str(upper_bound_z))\n",
    "\n",
    "    counter=0\n",
    "    for val in df.loc[:,hand_distances_key].lt(lower_bound):\n",
    "        if val:\n",
    "            counter+=1\n",
    "    for val in df.loc[:,hand_distances_key].gt(upper_bound):\n",
    "        if val:\n",
    "            counter+=1\n",
    "    counter_z=0\n",
    "    for val in df.loc[:,z_key].gt(upper_bound_z):\n",
    "        if val:\n",
    "            counter_z+=1\n",
    "    print(str(counter) + \" hand files, \"+str(counter_z)+\" z files\")\n",
    "\n",
    "for file in r_with_l:\n",
    "    fileNumber = file[20:28]\n",
    "    if differences[\"hands\"][fileNumber][file] > threshold[\"hands\"][fileNumber][1] or differences[\"hands\"][fileNumber][file] < threshold[\"hands\"][fileNumber][0]:\n",
    "        meanOfAllValues = pd.DataFrame(differences[\"hands\"][fileNumber].values()).mean()[0]\n",
    "        distanceToMean = abs(meanOfAllValues - differences[\"hands\"][fileNumber][file])\n",
    "        outliers[\"hand\"].update({file:distanceToMean})\n",
    "    if differences[\"z\"][fileNumber][file] > threshold[\"z\"][fileNumber]:\n",
    "        meanOfAllValues = pd.DataFrame(differences[\"z\"][fileNumber].values()).mean()[0]\n",
    "        distanceToMean = abs(meanOfAllValues - differences[\"z\"][fileNumber][file])\n",
    "        outliers[\"otherHand_z\"].update({file:distanceToMean})\n",
    "\n",
    "normalized_values = list(outliers[\"hand\"].values())\n",
    "max_normalized_value = max(normalized_values)\n",
    "min_normalized_value = min(normalized_values)\n",
    "for fileNumber in list(outliers[\"hand\"].keys()):\n",
    "    outliers[\"hand\"][fileNumber] = (outliers[\"hand\"][fileNumber]-min_normalized_value)/(max_normalized_value-min_normalized_value)\n",
    "\n",
    "normalized_values = list(outliers[\"otherHand_z\"].values())\n",
    "max_normalized_value = max(normalized_values)\n",
    "min_normalized_value = min(normalized_values)\n",
    "for fileNumber in list(outliers[\"otherHand_z\"].keys()):\n",
    "    outliers[\"otherHand_z\"][fileNumber] = (outliers[\"otherHand_z\"][fileNumber]-min_normalized_value)/(max_normalized_value-min_normalized_value)\n",
    "    \n",
    "for file in list(outliers_prefiltered[\"otherHand_z\"].keys()):\n",
    "    outliers[\"otherHand_z\"].update({file:outliers_prefiltered[\"otherHand_z\"][file]})\n",
    "for file in list(outliers_prefiltered[\"hand\"].keys()):\n",
    "    outliers[\"hand\"].update({file:outliers_prefiltered[\"hand\"][file]})\n",
    "    \n",
    "print(\"hand: \"+str(len(outliers[\"hand\"]))+\", z: \"+str(len(outliers[\"otherHand_z\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Schnittmenge\n",
    "Zu jedem Art Ausreißer werden zudem noch die normalized_values erfasst, die die Stärke des Ausreißer quantifizieren sollen. Sie stellen das Verhältnis des Abstands zum Mittelwert des Messdurchlaufs zum Mitelwert selbst dar und werden auf das Intervall 0-1 normiert.\n",
    "\n",
    "Broken files werden in den 99_broken Ordner geschrieben, im csv wird die Headerzeile durch eine Begründung der Aussortierung ergänzt. Angegeben werden die normierten Werte der Stärke der Ausreißer für z-Koordinate, Handabstände und Ellenbogenabstände sowie ob die Datei manuell aussortiert wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_files = {}\n",
    "pd.DataFrame({\"z-Werte\":list(outliers[\"otherHand_z\"].values())}).hist(figsize=(10,5),bins=40)\n",
    "pd.DataFrame({\"Handabstände\":list(outliers[\"hand\"].values())}).hist(figsize=(10,5),bins=40)\n",
    "pd.DataFrame({\"Ellenbogenabstände\":list(outliers[\"elbow\"].values())}).hist(figsize=(10,5),bins=80)\n",
    "\n",
    "\n",
    "#manual labeled files\n",
    "with open('manual_labeling/bad_files.csv') as f:\n",
    "    lines = f.readlines()\n",
    "manual_labeled_bad_files = [x.strip() for x in lines]\n",
    "manual_labeled_bad_files = [x.replace(\"raw\",\"relocated\") for x in manual_labeled_bad_files]\n",
    "manual_labeled_bad_files = ['../DATA/6_relocated/'+x for x in manual_labeled_bad_files]\n",
    "\n",
    "manual_broken = []\n",
    "for file in manual_labeled_bad_files:\n",
    "    if os.path.exists(file):\n",
    "        manual_broken.append(file)\n",
    "\n",
    "counters = []\n",
    "\n",
    "for file in all_files:\n",
    "    counter = 0\n",
    "    outlier_value_z = 0\n",
    "    outlier_value_hand = 0\n",
    "    outlier_value_elbow = 0\n",
    "    isManualBroken=False\n",
    "    if file in list(outliers[\"otherHand_z\"].keys()):\n",
    "        outlier_value_z = outliers[\"otherHand_z\"][file]\n",
    "        counter+=1+outlier_value_z\n",
    "    if file in list(outliers[\"hand\"].keys()):\n",
    "        outlier_value_hand = outliers[\"hand\"][file]\n",
    "        counter+=4*(1+outlier_value_hand)\n",
    "    if file in list(outliers[\"elbow\"].keys()):\n",
    "        outlier_value_elbow = outliers[\"elbow\"][file]\n",
    "        counter+=2*(1+outlier_value_elbow)\n",
    "    if file in manual_broken:\n",
    "        isManualBroken =True\n",
    "        #counter+=4.01\n",
    "    if counter != 0:\n",
    "        counters.append(counter)\n",
    "    if counter > 4:\n",
    "        broken_files.update({file:[outlier_value_z, outlier_value_hand, outlier_value_elbow, isManualBroken]})\n",
    "\n",
    "df = pd.DataFrame({\"Counter\":counters})\n",
    "df.hist(figsize=(10,5),bins=40)\n",
    "print(df.mean()[0])\n",
    "\n",
    "print(\"Broken: \"+str(len(broken_files.keys())))\n",
    "\n",
    "for f in os.listdir('../DATA/7_filtered'):\n",
    "    os.remove(os.path.join('../DATA/7_filtered', f))\n",
    "for f in os.listdir('../DATA/99_broken'):\n",
    "    os.remove(os.path.join('../DATA/99_broken', f))\n",
    "\n",
    "folder = '../DATA/6_relocated/'\n",
    "for file in os.listdir('../DATA/6_relocated'):\n",
    "    side = file[-5]\n",
    "    fileNumber = file.split(\"_relocated\",1)[0]\n",
    "    if folder+file in broken_files.keys():\n",
    "        z_value = round(broken_files[folder+file][0],3)\n",
    "        hand_value = round(broken_files[folder+file][1],3)\n",
    "        elbow_value = round(broken_files[folder+file][2],3)\n",
    "        is_manual_broken = broken_files[folder+file][3]\n",
    "        \n",
    "        broken_file = open(folder+file,'r')\n",
    "        appended_file = open('../DATA/99_broken/'+str(fileNumber)+'_broken_'+side+'.csv','w')\n",
    "        broken_content = broken_file.readlines()\n",
    "        appended_file.write('z-difference='+str(z_value)+', hand-distance='+str(hand_value)+', elbow-hand-distance='+str(elbow_value)+', manual broken:'+str(is_manual_broken)+'\\n')\n",
    "        for line in broken_content:\n",
    "            if line == broken_content[0]:\n",
    "                continue\n",
    "            appended_file.write(line)\n",
    "        broken_file.close()\n",
    "        appended_file.close()\n",
    "    else:\n",
    "        shutil.copyfile(folder+file,'../DATA/7_filtered/'+str(fileNumber)+'_filtered_'+side+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Rotation\n",
    "\n",
    "Trajektorien werden so rotiert, dass der Rotationspunkt auf der x-Achse liegt. Dieser ist z.B. der 70. Punkt der Bewegung. \n",
    "Rotationsachse: Normale der vom Ortsvektor des Midpoints und der x-Achse aufgespannten Ebene\n",
    "Rotationswinkel: Winkel zwischen Ortsvektor des Midpoints und x-Achse\n",
    "Rotation erfolgt über Drehmatrix (Koordinatentransformation), diese benötigt einen normierten Vektor als Drehachse und den Drehwinkel in Bogenmaß.\n",
    "\n",
    "Der Ortsvektor der ursprünglichen Trajektorie wird zur Retransformation in die erste Zeile der CSV-Datei geschrieben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_s = []\n",
    "\n",
    "ROTATION_POINT = 70\n",
    "\n",
    "for f in os.listdir('../DATA/8_rotated'):\n",
    "    os.remove(os.path.join('../DATA/8_rotated', f))\n",
    "\n",
    "for file in os.listdir('../DATA/7_filtered'):\n",
    "    fileNumber = file.split(\"_filtered\",1)[0]\n",
    "    side = file[-5]\n",
    "    df = pd.read_csv('../DATA/7_filtered/'+file, sep = ';') \n",
    "    if len(df) > (ROTATION_POINT-1):\n",
    "        rotation_point = df.iloc[(ROTATION_POINT-1),[0,1,2]] #get rotation point\n",
    "        cos_val = rotation_point.iloc[0] / (math.sqrt(rotation_point.iloc[0]**2+rotation_point.iloc[1]**2+rotation_point.iloc[2]**2)) #get angle to x-axis\n",
    "        a = math.acos(cos_val)\n",
    "\n",
    "        rotation_point_vec = np.array([[rotation_point.iloc[0]],[rotation_point.iloc[1]],[rotation_point.iloc[2]]])\n",
    "        x_vec = np.array([[1],[0],[0]])\n",
    "        norm_vec = np.cross(rotation_point_vec, x_vec, axis=0) #get normal vector of plane\n",
    "\n",
    "        n = np.divide(norm_vec, np.linalg.norm(norm_vec)) #normalize vector\n",
    "        n_s.append(n)\n",
    "\n",
    "        transform_mat = np.array([[n[0]**2*(1-cos(a))+cos(a),        n[0]*n[1]*(1-cos(a))-n[2]*sin(a), n[0]*n[2]*(1-cos(a))+n[1]*sin(a)],\n",
    "                                  [n[1]*n[0]*(1-cos(a))+n[2]*sin(a), n[1]**2*(1-cos(a))+cos(a),        n[1]*n[2]*(1-cos(a))-n[0]*sin(a)],\n",
    "                                  [n[2]*n[0]*(1-cos(a))-n[1]*sin(a), n[2]*n[1]*(1-cos(a))+n[0]*sin(a), n[2]**2*(1-cos(a))+cos(a)       ]])\n",
    "        transform_mat = transform_mat.reshape((3,3))\n",
    "        transformed_points = np.empty((0,3), float)\n",
    "\n",
    "        rotated_file = open('../DATA/8_rotated/'+fileNumber+'_rotated_'+side+'.csv','w')\n",
    "        rotated_file.write(f\"##rotation_point_vec:{rotation_point.iloc[0]};{rotation_point.iloc[1]};{rotation_point.iloc[2]}\\n\")\n",
    "\n",
    "        idx = 0\n",
    "        while idx<len(df):\n",
    "            point = np.array([[df.iloc[idx,0]],[df.iloc[idx,1]],[df.iloc[idx,2]]])\n",
    "            transformed_point = np.matmul(transform_mat, point)\n",
    "            transformed_point = np.transpose(transformed_point)\n",
    "            transformed_points = np.append(transformed_points, transformed_point, axis=0)\n",
    "            idx+=1\n",
    "\n",
    "        np.savetxt(rotated_file, transformed_points, fmt='%.4f', delimiter=';')\n",
    "        rotated_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sonstiger Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kopieren von kaputten Dateien einer Metrik\n",
    "Schreibt alle von einer Metrik als kaputt befundene DAteien in den Ordner 71_broken aus. Ist die Datei auch ingesamt als broken befunden wurden, stehen im Header Informationen zu allen Metriken, sonst nicht.\n",
    "Über drawAllBrokenFiles lassen sich die Dateien im 71_broken Ordner mit Header visualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC = 'elbow'  #'hand', 'otherHand_z'\n",
    "\n",
    "for f in os.listdir('../DATA/71_broken'):\n",
    "    os.remove(os.path.join('../DATA/71_broken', f))\n",
    "\n",
    "for file in outliers[METRIC].keys():\n",
    "    side = file[-5]\n",
    "    fileNumber = file.split(\"_relocated_\",1)[0]\n",
    "    fileNumber = fileNumber.split(\"6_relocated/\",1)[1]\n",
    "    metric_value = round(outliers[METRIC][file],3)\n",
    "    \n",
    "    if metric_value ==1:\n",
    "        if os.path.exists('../DATA/99_broken/'+fileNumber+'_broken_'+side+'.csv'):\n",
    "            shutil.copyfile('../DATA/99_broken/'+fileNumber+'_broken_'+side+'.csv', '../DATA/71_broken/'+fileNumber+'_broken_'+side+'.csv')\n",
    "        elif os.path.exists('../DATA/98_broken_prefiltered/'+fileNumber+'_broken_prefiltered_'+side+'.csv'):\n",
    "            shutil.copyfile('../DATA/98_broken_prefiltered/'+fileNumber+'_broken_prefiltered_'+side+'.csv', '../DATA/71_broken/'+fileNumber+'_broken_'+side+'.csv')\n",
    "        else:\n",
    "            broken_file = open(file,'r')\n",
    "            appended_file = open('../DATA/71_broken/'+str(fileNumber)+'_broken_'+side+'.csv','w')\n",
    "            broken_content = broken_file.readlines()\n",
    "            appended_file.write(METRIC+'-value='+str(metric_value)+'\\n')\n",
    "            for line in broken_content:\n",
    "                if line == broken_content[0]:\n",
    "                    continue\n",
    "                appended_file.write(line)\n",
    "            broken_file.close()\n",
    "            appended_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abgleich mit manuell gelabelten Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('manual_labeling/bad_files.csv') as f:\n",
    "    manual_labeled_bad_files = f.readlines()\n",
    "manual_labeled_bad_files = [x.strip() for x in manual_labeled_bad_files] \n",
    "prefiltered_files = []\n",
    "filtered_files = []\n",
    "len_bad_files = len(manual_labeled_bad_files)\n",
    "prefilteredCounter = 0\n",
    "filteredCounter = 0\n",
    "manPrefilteredCounter = 0\n",
    "manFilteredCounter = 0\n",
    "\n",
    "manualButNotFiltered = open('manual_labeling/manual_but_not_automatic.csv','w')\n",
    "filteredButNotManual = open('manual_labeling/automatic_but_not_manual.csv','w')\n",
    "\n",
    "for file in os.listdir('../DATA/98_broken_prefiltered'):\n",
    "    fileNumber = file.split(\"_broken\",1)[0]\n",
    "    side = file[-5]\n",
    "    fileName = fileNumber + '_raw_' +side+'.csv'\n",
    "    prefiltered_files.append(fileName)\n",
    "    if fileName in manual_labeled_bad_files:\n",
    "        prefilteredCounter += 1\n",
    "    else:\n",
    "        filteredButNotManual.write(fileName+'\\n')\n",
    "\n",
    "for file in os.listdir('../DATA/99_broken'):\n",
    "    fileNumber = file.split(\"_broken\",1)[0]\n",
    "    side = file[-5]\n",
    "    fileName = fileNumber + '_raw_' +side+'.csv'\n",
    "    filtered_files.append(fileName)\n",
    "    if fileName in manual_labeled_bad_files:\n",
    "        filteredCounter += 1\n",
    "    else:\n",
    "        filteredButNotManual.write(fileName+'\\n')\n",
    "\n",
    "for file in manual_labeled_bad_files:\n",
    "    if file in prefiltered_files:\n",
    "        manPrefilteredCounter+=1\n",
    "    if file in filtered_files:\n",
    "        manFilteredCounter+=1\n",
    "    if file not in prefiltered_files and file not in filtered_files:\n",
    "        manualButNotFiltered.write(file+'\\n')\n",
    "\n",
    "percentagePrefiltered = round(100*prefilteredCounter/len_bad_files,2)\n",
    "percentageFiltered = round(100*filteredCounter/len_bad_files,2)\n",
    "percentageManPrefilterd = round(100*manPrefilteredCounter/len(prefiltered_files),2)\n",
    "percentageManFiltered = round(100*manFilteredCounter/len(filtered_files),2)\n",
    "\n",
    "manualButNotFiltered.close()\n",
    "filteredButNotManual.close()\n",
    "\n",
    "print(f\"{percentagePrefiltered}% der manuell aussortierten Dateien ({prefilteredCounter}/{len_bad_files}) wurden auch präfiltert\")\n",
    "print(f\"{percentageFiltered}% der manuell aussortierten Dateien ({filteredCounter}/{len_bad_files}) wurden auch gefiltert\")\n",
    "print(\"--------------\")\n",
    "print(f\"{percentageManPrefilterd}% der präfilterten Dateien ({manPrefilteredCounter}/{len(prefiltered_files)}) wurden auch manuell aussortiert\")\n",
    "print(f\"{percentageManFiltered}% der gefilterten Dateien ({manFilteredCounter}/{len(filtered_files)}) wurden auch manuell aussortiert\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geschwindigkeitsstatistik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velo_stats = {\n",
    "    \"max_hand\":[],\n",
    "    \"mean_hand\":[],\n",
    "    \"max_elbow\":[],\n",
    "    \"mean_elbow\":[]\n",
    "}\n",
    "\n",
    "\n",
    "for file in r_files:\n",
    "    df = pd.read_csv(file, sep = ';')\n",
    "    v_df = np.gradient(df, axis=0)\n",
    "    velos_hand = np.sqrt(np.square(v_df[:,0])+np.square(v_df[:,1])+np.square(v_df[:,2]))\n",
    "    velos_elbow = np.sqrt(np.square(v_df[:,3])+np.square(v_df[:,4])+np.square(v_df[:,5]))\n",
    "    velo_stats[\"max_hand\"].append(np.max(velos_hand)*12)\n",
    "    velo_stats[\"mean_hand\"].append(np.mean(velos_hand)*12)\n",
    "    velo_stats[\"max_elbow\"].append(np.max(velos_elbow)*12)\n",
    "    velo_stats[\"mean_elbow\"].append(np.mean(velos_elbow)*12)\n",
    "\n",
    "pd.DataFrame(velo_stats).hist(figsize=(20, 10), bins=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaN-Statistik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zählt die NaN-Werte pro Datei im Ordner data und gibt die Anzahl der Dateien ohne NaN, mit bis zu 10 NaN oder mehr als 10 NaN aus. Die Dateinamen werden jeweils im Dictionary `files_list` in die entsprechende Liste hinzugefügt.   \n",
    "Es werden nur die Handgelenkdaten berücksichtigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = {\n",
    "    \"withoutNaN\" : [],\n",
    "    \"atleast2NaN\" : [],\n",
    "    \"morethan10NaN\": [],\n",
    "    \"worthToInterpolate\": []\n",
    "}\n",
    "\n",
    "for file in os.listdir('../DATA/0_raw'):\n",
    "    df = pd.read_csv('../DATA/0_raw/'+file, sep = ';')\n",
    "    wjc = df.iloc[:,[0,1,2]]\n",
    "    nans_sum = wjc.isna().sum().sum()\n",
    "\n",
    "    if nans_sum == 0:\n",
    "        files_list[\"withoutNaN\"].append(file)\n",
    "    elif nans_sum >= 1:\n",
    "        files_list[\"atleast2NaN\"].append(file)\n",
    "    if nans_sum > 10:\n",
    "        files_list[\"morethan10NaN\"].append(file)\n",
    "\n",
    "print(\"Files without NaN: \"+str(len(files_list[\"withoutNaN\"])))\n",
    "print(\"Files with at least 2 NaN: \"+str(len(files_list[\"atleast2NaN\"])))\n",
    "print(\"Files with more than 10 NaN: \"+str(len(files_list[\"morethan10NaN\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeichnet Histogramm über die Längen der NaN-Folgen in jeder Koordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaN_occurences = {\n",
    "    \"x\":[],\n",
    "    \"y\":[],\n",
    "    \"z\":[]\n",
    "}\n",
    "\n",
    "counter = {\n",
    "    \"x\":0,\n",
    "    \"y\":0,\n",
    "    \"z\":0\n",
    "}\n",
    "\n",
    "one = 0\n",
    "two = 0\n",
    "three = 0\n",
    "four = 0\n",
    "nan_seq = {}\n",
    "\n",
    "for file in os.listdir('../DATA/0_raw'):\n",
    "    df = pd.read_csv('../DATA/0_raw/'+file, sep = ';')  \n",
    "    wjc = df.iloc[:,[0,1,2]]\n",
    "    nan_sequence = [0]\n",
    "    for index,row in wjc.iterrows():\n",
    "        if math.isnan(row[0]):\n",
    "            counter[\"x\"]+=1\n",
    "        else:\n",
    "            if counter[\"x\"]!=0:\n",
    "                NaN_occurences[\"x\"].append(counter[\"x\"])\n",
    "                nan_sequence.append(counter[\"x\"])\n",
    "                counter[\"x\"]=0\n",
    "                \n",
    "        if math.isnan(row[1]):\n",
    "            counter[\"y\"]+=1\n",
    "        else:\n",
    "            if counter[\"y\"]!=0:\n",
    "                NaN_occurences[\"y\"].append(counter[\"y\"])\n",
    "                nan_sequence.append(counter[\"y\"])\n",
    "                counter[\"y\"]=0\n",
    "        \n",
    "        if math.isnan(row[2]):\n",
    "            counter[\"z\"]+=1\n",
    "        else:\n",
    "            if counter[\"z\"]!=0:\n",
    "                NaN_occurences[\"z\"].append(counter[\"z\"])\n",
    "                nan_sequence.append(counter[\"z\"])\n",
    "                counter[\"z\"]=0\n",
    "    if max(nan_sequence) <= 1:\n",
    "        one+=1\n",
    "    if max(nan_sequence) <= 2:\n",
    "        two+=1\n",
    "    if max(nan_sequence) <= 3:\n",
    "        three+=1\n",
    "    if max(nan_sequence) <= 4:\n",
    "        four+=1\n",
    "    nan_seq.update({file:max(nan_sequence)})\n",
    "    counter[\"x\"] = 0\n",
    "    counter[\"y\"] = 0\n",
    "    counter[\"z\"] = 0\n",
    "\n",
    "nans_df = pd.DataFrame(NaN_occurences)\n",
    "        \n",
    "pd.DataFrame(nan_seq.values()).hist(figsize=(30,20), bins=175)\n",
    "nans_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dist = {}\n",
    "\n",
    "for val in nan_seq.values():\n",
    "    for idx in range(165):\n",
    "        if val <= idx:\n",
    "            if idx in dist:\n",
    "                dist.update({idx:dist[idx]+1})\n",
    "            else:\n",
    "                dist.update({idx:1})\n",
    "\n",
    "for key in dist.keys():\n",
    "    dist.update({key:round(100*dist[key]/1170,2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(dist.values())\n",
    "plt.xlabel('Längste NaN-Folge')\n",
    "plt.ylabel('Anteil Datensätze in %')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(list(dist.values())[:10])\n",
    "plt.axvline(x = 3, color = 'r') \n",
    "#plt.axhline(y = 84.02, color = 'r') \n",
    "plt.xlabel('Längste NaN-Folge')\n",
    "plt.ylabel('Anteil Datensätze in %')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestalt der Kurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../DATA/0_raw/20200423235_raw_L.csv', sep=';')\n",
    "\n",
    "x_values = list(df.iloc[:,0])\n",
    "y_values = list(df.iloc[:,1])\n",
    "z_values = list(df.iloc[:,2])\n",
    "plt.plot(x_values)\n",
    "plt.plot(y_values)\n",
    "plt.plot(z_values)\n",
    "plt.title(\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fügt nach Festlegung eines Schwellwertes in \"worthToInterpolate\"-Liste alle Dateinamen hinzu, die maximal soviele direkt aufeinanderfolgende NaNs haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = {\n",
    "    \"x\":0,\n",
    "    \"y\":0,\n",
    "    \"z\":0\n",
    "}\n",
    "threshold = 2\n",
    "files_list[\"worthToInterpolate\"].clear()\n",
    "\n",
    "for file in files_list[\"atleast1NaN\"]:\n",
    "    df = pd.read_csv('./data/'+file, sep = ';')  \n",
    "    wjc = df.iloc[:,[0,1,2]]\n",
    "    \n",
    "    row = 0\n",
    "    while counter[\"x\"] <= threshold and counter[\"y\"] <= threshold and counter[\"z\"] <= threshold:\n",
    "        if math.isnan(wjc.iloc[row,0]):\n",
    "            counter[\"x\"]+=1\n",
    "        if math.isnan(wjc.iloc[row,1]):\n",
    "            counter[\"y\"]+=1\n",
    "        if math.isnan(wjc.iloc[row,2]):\n",
    "            counter[\"z\"]+=1\n",
    "        if row == (len(wjc)-1):\n",
    "            files_list[\"worthToInterpolate\"].append(file)\n",
    "            break\n",
    "        else:\n",
    "            row+=1\n",
    "    counter[\"x\"] = 0\n",
    "    counter[\"y\"] = 0\n",
    "    counter[\"z\"] = 0\n",
    "\n",
    "\n",
    "print(\"Files with max. \"+str(threshold)+\" NaNs in a row: \"+str(len(files_list[\"worthToInterpolate\"])))\n",
    "\n",
    "print(files_list[\"worthToInterpolate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startpunkte analysieren\n",
    "Visualisiert die verschiedenen Startkoordinaten für x, y und z in drei Histogrammen sowie berechnet statistische Größen (für die rechte Hand).\n",
    "\n",
    "Ausreißer: 20200423006_takeover_RGraspPhase.csv, 20200305163_takeover_RGraspPhase.csv (z)\n",
    "\n",
    "Mehrere Hügel durch mehrere Messdurchläufe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = {\n",
    "    \"x\": [],\n",
    "    \"y\": [],\n",
    "    \"z\": []\n",
    "}\n",
    "\n",
    "for file in files_list[\"withoutNaN\"]:\n",
    "    df = pd.read_csv('./data/'+file, sep = ';')\n",
    "    starts[\"x\"].append(df.iloc[0,0])\n",
    "    starts[\"y\"].append(df.iloc[0,1])\n",
    "    starts[\"z\"].append(df.iloc[0,2])\n",
    "    \n",
    "ser = pd.DataFrame(starts)\n",
    "ser.hist(figsize=(20, 10), bins=50)\n",
    "\n",
    "ser.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "a92cffb903fb97e8873a613ac605a3ffa5627076f8a836ecc732e4fee6d5cfd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
