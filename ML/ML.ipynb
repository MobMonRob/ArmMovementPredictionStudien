{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End-Anwendung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pandas as pd, numpy as np, tensorflow as tf, math, random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import cos, sin\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpunkt-Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENUMBER = '20200403155'\n",
    "\n",
    "df = pd.read_csv('../DATA/3_truncated/'+FILENUMBER+'_truncated_R.csv', sep = ';').iloc[:85,[0,1,2]]\n",
    "end_point = pd.read_csv('../DATA/3_truncated/'+FILENUMBER+'_truncated_R.csv', sep = ';').iloc[-1,[0,1,2]]\n",
    "\n",
    "#start time\n",
    "begin_time = time.time_ns()\n",
    "#relocation\n",
    "pos_original = df.iloc[0,[0,1,2]]\n",
    "relocated = pd.DataFrame(df.iloc[:,0]-df.iloc[0,0])\n",
    "relocated[df.columns.values[1]] = df.iloc[:,1]-df.iloc[0,1]\n",
    "relocated[df.columns.values[2]] = df.iloc[:,2]-df.iloc[0,2]     \n",
    "\n",
    "#rotation\n",
    "mid_point = relocated.iloc[(69),[0,1,2]] #get midpoint\n",
    "cos_val = mid_point.iloc[0] / (math.sqrt(mid_point.iloc[0]**2+mid_point.iloc[1]**2+mid_point.iloc[2]**2)) #get angle to x-axis\n",
    "a = math.acos(cos_val)\n",
    "\n",
    "mid_point_vec = np.array([[mid_point.iloc[0]],[mid_point.iloc[1]],[mid_point.iloc[2]]])\n",
    "x_vec = np.array([[1],[0],[0]])\n",
    "norm_vec = np.cross(mid_point_vec, x_vec, axis=0) #get normal vector of plane\n",
    "\n",
    "n = np.divide(norm_vec, np.linalg.norm(norm_vec)) #normalize vector\n",
    "transform_mat = np.array([[n[0]**2*(1-cos(a))+cos(a),        n[0]*n[1]*(1-cos(a))-n[2]*sin(a), n[0]*n[2]*(1-cos(a))+n[1]*sin(a)],\n",
    "                          [n[1]*n[0]*(1-cos(a))+n[2]*sin(a), n[1]**2*(1-cos(a))+cos(a),        n[1]*n[2]*(1-cos(a))-n[0]*sin(a)],\n",
    "                          [n[2]*n[0]*(1-cos(a))-n[1]*sin(a), n[2]*n[1]*(1-cos(a))+n[0]*sin(a), n[2]**2*(1-cos(a))+cos(a)       ]])\n",
    "transform_mat = transform_mat.reshape((3,3))\n",
    "transformed_points = np.empty((0,3), float)\n",
    "\n",
    "idx = 0\n",
    "while idx<len(relocated):\n",
    "    point = np.array([[relocated.iloc[idx,0]],[relocated.iloc[idx,1]],[relocated.iloc[idx,2]]])\n",
    "    transformed_point = np.matmul(transform_mat, point)\n",
    "    transformed_point = np.transpose(transformed_point)\n",
    "    transformed_points = np.append(transformed_points, transformed_point, axis=0)\n",
    "    idx+=1\n",
    "\n",
    "input_ml = transformed_points.flatten().reshape(1,-1)\n",
    "\n",
    "#prediction\n",
    "prediction = scaler_y.inverse_transform(model.predict(scaler_x.transform(input_ml)))\n",
    "\n",
    "#rerotate\n",
    "a = -a\n",
    "transform_mat = np.array([[n[0]**2*(1-cos(a))+cos(a),        n[0]*n[1]*(1-cos(a))-n[2]*sin(a), n[0]*n[2]*(1-cos(a))+n[1]*sin(a)],\n",
    "                          [n[1]*n[0]*(1-cos(a))+n[2]*sin(a), n[1]**2*(1-cos(a))+cos(a),        n[1]*n[2]*(1-cos(a))-n[0]*sin(a)],\n",
    "                          [n[2]*n[0]*(1-cos(a))-n[1]*sin(a), n[2]*n[1]*(1-cos(a))+n[0]*sin(a), n[2]**2*(1-cos(a))+cos(a)       ]])\n",
    "transform_mat = transform_mat.reshape((3,3))\n",
    "\n",
    "rotated_pred = np.matmul(transform_mat, prediction.reshape(3,1))\n",
    "\n",
    "print(rotated_pred)\n",
    "\n",
    "#relocate\n",
    "rotated_pred[0] = rotated_pred[0]+pos_original[0]\n",
    "rotated_pred[1] = rotated_pred[1]+pos_original[1]\n",
    "rotated_pred[2] = rotated_pred[2]+pos_original[2]\n",
    "\n",
    "end_time = time.time_ns()\n",
    "\n",
    "print(rotated_pred)\n",
    "print(end_point)\n",
    "runtime = round((end_time - begin_time) / 1000000,3)\n",
    "print(f\"Total runtime: {runtime} ms \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dauer-Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENUMBER = '20200305325'\n",
    "\n",
    "df = pd.read_csv('../DATA/3_truncated/'+FILENUMBER+'_truncated_R.csv', sep = ';').iloc[:85,[0,1,2]]\n",
    "duration = len(df)\n",
    "\n",
    "input_ml = df.values.flatten().reshape(1,-1)\n",
    "\n",
    "prediction = scaler_y.inverse_transform(model.predict(scaler_x.transform(input_ml)))\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpunkt-Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_rotation_points = [70]\n",
    "possible_limits = [85]\n",
    "\n",
    "possible_epochs = [350]\n",
    "possible_batch_sizes = [250]\n",
    "possible_test_sizes = [0.25]\n",
    "\n",
    "COST_FUNC = 'euclidean' #1\n",
    "\n",
    "def euclid_loss(y_true, y_pred):\n",
    "    euclid = tf.square(tf.norm(y_true-y_pred,ord=COST_FUNC, axis=-1))\n",
    "    return euclid\n",
    "\n",
    "for EPOCHS in possible_epochs:\n",
    "    for BATCH_SIZE in possible_batch_sizes:\n",
    "        for TEST_SIZE in possible_test_sizes:\n",
    "            for ROTATION_POINT in possible_rotation_points:\n",
    "                for LIMIT in possible_limits:\n",
    "                    VAL_SIZE = 0.05\n",
    "                    #DATA = '8_'+str(ROTATION_POINTske)+'_rotated'\n",
    "                    DATA = '8_rotated'\n",
    "                    x_train = np.empty([0,3*LIMIT])\n",
    "                    x_test = np.empty([0,3*LIMIT])\n",
    "                    y_train = np.empty([0,3])\n",
    "                    y_test = np.empty([0,3])\n",
    "\n",
    "\n",
    "                    for file in os.listdir('../DATA/'+DATA+'/'):\n",
    "                        df = pd.read_csv('../DATA/'+DATA+'/'+file, sep = ';').iloc[:,[0,1,2]]\n",
    "                        if len(df) >=LIMIT+20:\n",
    "                            x_row = df.iloc[0:LIMIT].values.flatten()\n",
    "                            y_row = df.iloc[-1].values.flatten()\n",
    "                            x_train = np.vstack([x_train, x_row])\n",
    "                            y_train = np.vstack([y_train, y_row])\n",
    "\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=TEST_SIZE)\n",
    "\n",
    "                    scaler_x = MinMaxScaler()\n",
    "                    scaler_y = MinMaxScaler()\n",
    "\n",
    "                    xtrain_scale=scaler_x.fit_transform(X_train)\n",
    "                    xtest_scale=scaler_x.fit_transform(X_test)\n",
    "                    ytrain_scale=scaler_y.fit_transform(y_train)\n",
    "                    ytest_scale=scaler_y.fit_transform(y_test)\n",
    "\n",
    "                    inputs = keras.Input(shape=(3*LIMIT,))\n",
    "                    x = layers.Dense(3*LIMIT, input_dim=3*LIMIT, activation=\"relu\")(inputs)\n",
    "                    x = layers.Dense(0.5*LIMIT, activation=\"relu\")(x)\n",
    "                    outputs = layers.Dense(3, activation=\"linear\")(x)\n",
    "                    model = keras.Model(inputs=inputs, outputs=outputs, name=\"hand_prediction\")\n",
    "                    model.summary()\n",
    "\n",
    "                    model.compile(loss=euclid_loss, optimizer='adam', metrics=['mse','mae','accuracy'])\n",
    "\n",
    "                    history = model.fit(xtrain_scale,ytrain_scale, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0, validation_split=VAL_SIZE)\n",
    "                    predictions = model.predict(xtest_scale)\n",
    "\n",
    "                    score = model.evaluate(xtest_scale, ytest_scale, verbose=0)\n",
    "\n",
    "                    #euclid metric test_Data\n",
    "                    predictions = scaler_y.inverse_transform(model.predict(xtest_scale))\n",
    "                    x = pd.DataFrame(predictions)\n",
    "                    y = pd.DataFrame(y_test)\n",
    "                    distance_pred_real = []\n",
    "                    for i in range(min(len(x),len(y))):\n",
    "                        distance_pred_real.append(math.sqrt((x.iloc[i,0]-y.iloc[i,0])**2 + (x.iloc[i,1]-y.iloc[i,1])**2 + (x.iloc[i,2]-y.iloc[i,2])**2))\n",
    "                    euclid_metric_test = round(mean(distance_pred_real),3)\n",
    "                    number_of_test_data = len(distance_pred_real)\n",
    "                    print(f\"Test:{euclid_metric_test}, Anzahl Datens√§tze: {number_of_test_data}\")\n",
    "\n",
    "                    #euclid metric train data\n",
    "                    predictions = scaler_y.inverse_transform(model.predict(xtrain_scale))\n",
    "                    x = pd.DataFrame(predictions)\n",
    "                    y = pd.DataFrame(y_train)\n",
    "                    distance_pred_real = []\n",
    "                    for i in range(min(len(x),len(y))):\n",
    "                        distance_pred_real.append(math.sqrt((x.iloc[i,0]-y.iloc[i,0])**2 + (x.iloc[i,1]-y.iloc[i,1])**2 + (x.iloc[i,2]-y.iloc[i,2])**2))\n",
    "                    euclid_metric_train = round(mean(distance_pred_real),3)\n",
    "                    number_of_train_data = len(distance_pred_real)\n",
    "                    print(f\"Train:{euclid_metric_train}, Anzahl Datens√§tze: {number_of_train_data}\")\n",
    "\n",
    "                    #visualize\n",
    "                    plt.plot(history.history[\"loss\"])\n",
    "                    plt.plot(history.history['val_loss'])\n",
    "                    plt.title('model loss')\n",
    "                    plt.ylabel('loss')\n",
    "                    plt.xlabel('epoch')\n",
    "                    plt.legend(['train', 'validation'], loc='upper left')\n",
    "                    plt.show()\n",
    "\n",
    "                    #export data\n",
    "                    model_name = f\"{euclid_metric_test}_{euclid_metric_train}.json\"\n",
    "                    print(\"Model name: \"+model_name)\n",
    "                    model_file = open('../ML/models_overview.csv','a')\n",
    "                    model_file.write(f\"\\n{LIMIT};{number_of_test_data};{number_of_train_data};{euclid_metric_test};{euclid_metric_train};{str(round(score[1],5))};{str(round(mean(history.history['mse']),5))};{str(round(score[2],5))};{str(round(mean(history.history['mae']),5))};{str(round(score[3],5))};{str(round(mean(history.history['accuracy']),5))};{model_name};{DATA};{EPOCHS};{BATCH_SIZE};{TEST_SIZE};{VAL_SIZE};{COST_FUNC}\")\n",
    "                    model_file.close()\n",
    "\n",
    "                    model_json_file = open('../ML/model_configurations/'+model_name,'w')\n",
    "                    model_json_file.write(model.to_json())\n",
    "                    model_json_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dauer-Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_rotation_points = [70]\n",
    "possible_limits = [85]\n",
    "\n",
    "possible_epochs = [100]\n",
    "possible_batch_sizes = [200]\n",
    "possible_test_sizes = [0.25]\n",
    "\n",
    "COST_FUNC = 'mse' #1\n",
    "\n",
    "for EPOCHS in possible_epochs:\n",
    "    for BATCH_SIZE in possible_batch_sizes:\n",
    "        for TEST_SIZE in possible_test_sizes:\n",
    "            for ROTATION_POINT in possible_rotation_points:\n",
    "                for LIMIT in possible_limits:\n",
    "                    VAL_SIZE = 0.10\n",
    "                    #DATA = '8_'+str(MID_POINT)+'_rotated'\n",
    "                    DATA = '8_rotated'\n",
    "                    x_train = np.empty([0,3*LIMIT])\n",
    "                    x_test = np.empty([0,3*LIMIT])\n",
    "                    y_train = np.empty([0,1])\n",
    "                    y_test = np.empty([0,1])\n",
    "\n",
    "\n",
    "                    for file in os.listdir('../DATA/'+DATA+'/'):\n",
    "                        df = pd.read_csv('../DATA/'+DATA+'/'+file, sep = ';').iloc[:,[0,1,2]]\n",
    "                        if len(df) >=LIMIT+20:\n",
    "                            x_row = df.iloc[0:LIMIT].values.flatten()\n",
    "                            y_row = len(df)\n",
    "                            x_train = np.vstack([x_train, x_row])\n",
    "                            y_train = np.vstack([y_train, y_row])\n",
    "\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=TEST_SIZE)\n",
    "\n",
    "                    scaler_x = MinMaxScaler()\n",
    "                    scaler_y = MinMaxScaler()\n",
    "\n",
    "                    xtrain_scale=scaler_x.fit_transform(X_train)\n",
    "                    xtest_scale=scaler_x.fit_transform(X_test)\n",
    "                    ytrain_scale=scaler_y.fit_transform(y_train)\n",
    "                    ytest_scale=scaler_y.fit_transform(y_test)\n",
    "\n",
    "                    inputs = keras.Input(shape=(3*LIMIT,))\n",
    "                    x = layers.Dense(LIMIT, input_dim=3*LIMIT, activation=\"relu\")(inputs)\n",
    "                    outputs = layers.Dense(1, activation=\"linear\")(x)\n",
    "                    model = keras.Model(inputs=inputs, outputs=outputs, name=\"hand_prediction\")\n",
    "\n",
    "                    model.compile(loss='mae', optimizer='adam', metrics=['mse','mae','accuracy'])\n",
    "\n",
    "                    history = model.fit(xtrain_scale,ytrain_scale, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0, validation_split=VAL_SIZE)\n",
    "                    predictions = model.predict(xtest_scale)\n",
    "\n",
    "                    score = model.evaluate(xtest_scale, ytest_scale, verbose=0)\n",
    "\n",
    "                    print('mse test: ', score[1], '    training: ', mean(history.history['mse']))\n",
    "                    print('mae test: ', score[2], '    training: ', mean(history.history['mae']))\n",
    "                    print('accuracy test: ', score[3], '    training: ', mean(history.history['accuracy']))\n",
    "\n",
    "                    #test\n",
    "                    predictions = scaler_y.inverse_transform(model.predict(xtest_scale))\n",
    "                    y = pd.DataFrame(y_test)\n",
    "                    abs_distances = []\n",
    "                    y = y.values.tolist()\n",
    "                    number_of_test_data = len(X_test)\n",
    "                    for i in range(number_of_test_data-1):\n",
    "                        diff1 = abs(predictions[i] - y[i][0])[0]\n",
    "                        abs_distances.append(diff1)\n",
    "                    print(f'Test mean distance: {round(pd.Series(abs_distances).mean(),2)}')\n",
    "                    \n",
    "                    #train\n",
    "                    predictions = scaler_y.inverse_transform(model.predict(xtrain_scale))\n",
    "                    y = pd.DataFrame(y_train)\n",
    "                    abs_distances = []\n",
    "                    y = y.values.tolist()\n",
    "                    number_of_train_data = len(X_train)\n",
    "                    for i in range(number_of_train_data-1):\n",
    "                        diff1 = abs(predictions[i] - y[i][0])[0]\n",
    "                        abs_distances.append(diff1)\n",
    "                    print(f'Train mean distance: {round(pd.Series(abs_distances).mean(),2)}')\n",
    "\n",
    "\n",
    "                    #visualize\n",
    "                    plt.plot(history.history[\"loss\"])\n",
    "                    plt.plot(history.history['val_loss'])\n",
    "                    plt.title('model loss')\n",
    "                    plt.ylabel('loss')\n",
    "                    plt.xlabel('epoch')\n",
    "                    plt.legend(['train', 'validation'], loc='upper left')\n",
    "                    plt.show()\n",
    "\n",
    "                    #export data\n",
    "                    euclid_metric_test = 'time-regression'\n",
    "                    euclid_metric_train = round(score[0],5)\n",
    "                    number_of_train_data = len(x_train)\n",
    "                    number_of_test_data = len(X_test)\n",
    "                    \n",
    "                    \n",
    "                    model_name = f\"{euclid_metric_test}_{euclid_metric_train}.json\"\n",
    "                    print(\"Model name: \"+model_name)\n",
    "                    model_file = open('../ML/models_overview.csv','a')\n",
    "                    model_file.write(f\"\\n{LIMIT};{number_of_test_data};{number_of_train_data};{euclid_metric_test};{euclid_metric_train};{str(round(score[1],5))};{str(round(mean(history.history['mse']),5))};{str(round(score[2],5))};{str(round(mean(history.history['mae']),5))};{str(round(score[3],5))};{str(round(mean(history.history['accuracy']),5))};{model_name};{DATA};{EPOCHS};{BATCH_SIZE};{TEST_SIZE};{VAL_SIZE};{COST_FUNC}\")\n",
    "                    model_file.close()\n",
    "\n",
    "                    model_json_file = open('../ML/model_configurations/'+model_name,'w')\n",
    "                    model_json_file.write(model.to_json())\n",
    "                    model_json_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "abs_distances = []\n",
    "\n",
    "#y = y.values.tolist()\n",
    "\n",
    "number_of_test_data = len(X_test)\n",
    "for i in range(number_of_test_data):\n",
    "    #val = random.randint(0,(number_of_test_data-1))\n",
    "    #print(predictions[i])\n",
    "    #print(y.iloc[i][0])\n",
    "    diff1 = abs(predictions[i] - y[i][0])[0]\n",
    "    abs_distances.append(diff1)\n",
    "    #print(f\"Differenz: \"+str(diff1))\n",
    "\n",
    "print(round(pd.Series(abs_distances).mean(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ergebnisvisualisierung Endpunkte\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung des Testdatensatzes\n",
    "Visualisierung der Predictions je Koordinate auf x-Achse und tats√§chlichen Werte auf y-Achse, Punkte sollten idealerweise auf roter Urpsprungsgeraden liegen.\n",
    "Viertes Koordinatesystem zeigt die L√§nge der Ortsvektoren der Punkte (als Aggregation √ºber x-, y- und z-Koordinate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xtrain_scale\n",
    "y = y_train\n",
    "\n",
    "predictions = scaler_y.inverse_transform(model.predict(x))\n",
    "\n",
    "predicted_length = list()\n",
    "real_length = list()\n",
    "\n",
    "for val in predictions:\n",
    "    predicted_length.append(math.sqrt(val[0]**2 + val[1]**2 + val[2]**2))\n",
    "for val in y:\n",
    "    real_length.append(math.sqrt(val[0]**2 + val[1]**2 + val[2]**2))\n",
    "                                       \n",
    "for i in range(4):\n",
    "    if i == 3:\n",
    "        predicted_data = predicted_length\n",
    "        real_data = real_length\n",
    "    else:\n",
    "        predicted_data = [item[i] for item in predictions]\n",
    "        real_data = [item[i] for item in y]\n",
    "\n",
    "    min_value = min(min(predicted_data), min(real_data)) - 25\n",
    "    max_value = max(max(predicted_data), max(real_data)) + 25\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(predicted_data,real_data)\n",
    "    \n",
    "    if i == 0:\n",
    "        coord = 'x-Koordinate'\n",
    "    elif i == 1:\n",
    "        coord = 'y-Koordinate'\n",
    "    elif i == 2:\n",
    "        coord = 'z-Koordinate'\n",
    "    elif i == 3: \n",
    "        coord = 'euklidische Norm'\n",
    "    plt.ylabel('tats√§chliche '+coord)\n",
    "    plt.xlabel('vorhergesagte '+coord)\n",
    "    plt.xlim(min_value, max_value)\n",
    "    plt.ylim(min_value, max_value)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.plot([min_value,max_value],[min_value,max_value], color = 'r')\n",
    "    plt.plot([min_value-100,max_value-100],[min_value+100,max_value+100], color = 'y')   \n",
    "    plt.plot([min_value+100,max_value+100],[min_value-100,max_value-100], color = 'y')\n",
    "    plt.show()\n",
    "\n",
    "x = xtest_scale\n",
    "y = y_test \n",
    "\n",
    "predictions = scaler_y.inverse_transform(model.predict(x))\n",
    "\n",
    "predicted_length = list()\n",
    "real_length = list()\n",
    "\n",
    "for val in predictions:\n",
    "    predicted_length.append(math.sqrt(val[0]**2 + val[1]**2 + val[2]**2))\n",
    "for val in y:\n",
    "    real_length.append(math.sqrt(val[0]**2 + val[1]**2 + val[2]**2))\n",
    "                                       \n",
    "for i in range(4):\n",
    "    if i == 3:\n",
    "        predicted_data = predicted_length\n",
    "        real_data = real_length\n",
    "    else:\n",
    "        predicted_data = [item[i] for item in predictions]\n",
    "        real_data = [item[i] for item in y]\n",
    "\n",
    "    min_value = min(min(predicted_data), min(real_data)) - 25\n",
    "    max_value = max(max(predicted_data), max(real_data)) + 25\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(predicted_data,real_data)\n",
    "    \n",
    "    \n",
    "    if i == 0:\n",
    "        coord = 'x-Koordinate'\n",
    "    elif i == 1:\n",
    "        coord = 'y-Koordinate'\n",
    "    elif i == 2:\n",
    "        coord = 'z-Koordinate'\n",
    "    elif i == 3: \n",
    "        coord = 'euklidische Norm'\n",
    "    plt.ylabel('tats√§chliche '+coord)\n",
    "    plt.xlabel('vorhergesagte '+coord)\n",
    "    plt.xlim(min_value, max_value)\n",
    "    plt.ylim(min_value, max_value)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.plot([min_value,max_value],[min_value,max_value], color = 'r')\n",
    "    plt.plot([min_value-100,max_value-100],[min_value+100,max_value+100], color = 'y')   \n",
    "    plt.plot([min_value+100,max_value+100],[min_value-100,max_value-100], color = 'y')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "for i in range(3):\n",
    "    val = random.randint(0,(number_of_test_data-1))\n",
    "    print(predictions[val])\n",
    "    print(y[val])\n",
    "    diff1 = abs(predictions[val,0] - y[val,0])\n",
    "    print(f\"Differenzen: {abs(predictions[val,0] - y[val,0])}, {abs(predictions[val,1] - y[val,1])}, {abs(predictions[val,2] - y[val,2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export der vorhergesagten Endpunkte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_real_endpoints = open('../ML/predictions/predicted_real_endpoints_'+model_name+'.csv','w')\n",
    "predicted_real_endpoints.write(\"pred-x;pred-y;pred-z;real-x;real-y;real-z\\n\")\n",
    "\n",
    "idx = 0\n",
    "while idx<len(predictions) and idx<len(y):\n",
    "    predicted_real_endpoints.write(f\"{predictions[idx,0]};{predictions[idx,1]};{predictions[idx,2]};{y[idx,0]};{y[idx,1]};{y[idx,2]}\\n\")\n",
    "    idx+=1\n",
    "\n",
    "predicted_real_endpoints.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung der Metrikentwicklungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for metric in ['loss','mse', 'mae', 'accuracy']:\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_'+metric])\n",
    "    plt.title('model '+metric)\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "predictions = scaler_y.inverse_transform(predictions)\n",
    "y_test = scaler_y.inverse_transform(ytest_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fache Kreuzvalidierung\n",
    "Realit√§tsn√§her als einfache Teststrategie mit statischem Testdatensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pandas as pd, numpy as np, tensorflow as tf\n",
    "from numpy import mean, std\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "LIMIT = 80\n",
    "\n",
    "X = np.empty([0,3*LIMIT])\n",
    "y = np.empty([0,3])\n",
    "\n",
    "for file in os.listdir('../DATA/5_filtered'):\n",
    "    if file.find(\"R\") != -1:\n",
    "        df = pd.read_csv('../DATA/5_filtered/'+file, sep = ';').iloc[:,[0,1,2]]\n",
    "        if len(df) >=LIMIT+20:\n",
    "            x_row = df.iloc[0:LIMIT].values.flatten()\n",
    "            y_row = df.iloc[-1].values.flatten()\n",
    "            X = np.vstack([X, x_row])\n",
    "            y = np.vstack([y, y_row])\n",
    "\n",
    "#k-fold cross-validation\n",
    "results = list()\n",
    "n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=2, random_state=1)\n",
    "\n",
    "for train_ix, test_ix in cv.split(X):\n",
    "    X_train, X_test = X[train_ix], X[test_ix]\n",
    "    y_train, y_test = y[train_ix], y[test_ix]\n",
    "    \n",
    "    #normalize values\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    xtrain_scale=scaler_x.fit_transform(X_train)\n",
    "    xtest_scale=scaler_x.fit_transform(X_test)\n",
    "    ytrain_scale=scaler_y.fit_transform(y_train)\n",
    "    ytest_scale=scaler_y.fit_transform(y_test)\n",
    "    \n",
    "    #define model\n",
    "    inputs = keras.Input(shape=(3*LIMIT,))\n",
    "    x = layers.Dense(3*LIMIT, input_dim=3*LIMIT, activation=\"relu\")(inputs)\n",
    "    x = layers.Dense(3*LIMIT*(3*LIMIT+3), activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(3, activation=\"linear\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"hand_prediction\")\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae','accuracy'])\n",
    "    \n",
    "    #fit model\n",
    "    history = model.fit(xtrain_scale,ytrain_scale, epochs=200, verbose=0)\n",
    "    predictions = model.predict(xtest_scale)\n",
    "    \n",
    "    #evaluate mode on test set\n",
    "    score = model.evaluate(xtest_scale, ytest_scale, verbose=0)\n",
    "    results.append(score)\n",
    "    print('----------------------------------------')\n",
    "    print('Test mse:', round(score[1],3))\n",
    "    print('Test mae:', round(score[2],3))\n",
    "    print('Test accuracy:', round(score[3],3))\n",
    "    \n",
    "print('=========================================')\n",
    "\n",
    "mse = [item[1] for item in results]\n",
    "mae = [item[2] for item in results]\n",
    "accuracy = [item[3] for item in results]\n",
    "print(\"mse: \"+str(mean(mse))+\" (std: \"+str(std(mse))+\")\")\n",
    "print(\"mae: \"+str(mean(mae))+\" (std: \"+str(std(mae))+\")\")\n",
    "print(\"accuracy: \"+str(mean(accuracy))+\" (std: \"+str(std(accuracy))+\")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "a92cffb903fb97e8873a613ac605a3ffa5627076f8a836ecc732e4fee6d5cfd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
